{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto_encoder1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akif-Mufti/Deep-learning-2.0/blob/master/Auto_encoder1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7YViSPL_qzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dependencies\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from urllib import request\n",
        "import random\n",
        "\n",
        "#importing the dataset\n",
        "\n",
        "#url https://www.kaggle.com/zalando-research/fashionmnist\n",
        "\n",
        "img_database = np.loadtxt('fashion-mnist_train.csv',delimiter =',',skiprows =1)\n",
        "#looking at the shape of the file\n",
        "print(image_database.shape)\n",
        "total_num_images = (img_database.shape[0])\n",
        "\n",
        "#defining the neural network \n",
        "\n",
        "n_input = 784 #image of size 28 X28\n",
        "n_hidden_1 = 256 #first hidden layer\n",
        "n_hidden_2 = 32 #second hidden layer\n",
        "n_hidden_3 = 32 #third hidden layer\n",
        "n_hidden_4 = 256 #fourth hidden layer\n",
        "n_output = 784  \n",
        "\n",
        "#hyper parameters (remain constant throughout the process\n",
        "\n",
        "learning_rate = .1 #how the parameters will be adjusted at each step of the learning process\n",
        "epochs = 3000 #how many times we go through the training process\n",
        "batch_size = 100 #how many training examples we are using at each step \n",
        "keep_prop = tf.placeholder(tf.float32)\n",
        "\n",
        "#building tensorflow graph\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_input]) #[none, 784] [any number of training sets ]\n",
        "Y = tf.placeholder(tf.float32, [None, n_output]) #[none, 10 ]\n",
        "\n",
        "#defining the weights and biases\n",
        "\n",
        "nn_weight = {\"W1\": tf.Variable(tf.truncated_normal([n_input,n_hidden_1])),\n",
        "             \"W2\": tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2])),\n",
        "             \"W3\": tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3])),\n",
        "             \"W4\": tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4])),\n",
        "             \"Wout\": tf.Variable(tf.truncated_normal([n_hidden_3,n_output])\n",
        "            }\n",
        "nn_bias = {\n",
        "           \"B1\": tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
        "           \"B2\": tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
        "           \"B3\": tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
        "           \"B4\": tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
        "           \"B5\": tf.Variable(tf.truncated_normal([n_output]))\n",
        "            }\n",
        "\n",
        "#creating a computatioal graph \n",
        "\n",
        "#for neural network\n",
        "z1 = tf.add(tf.matmul(X, Weight_NN[\"W1\"]), Bias_NN[\"B1\"])\n",
        "z1_out = tf.nn.sigmoid(z1)\n",
        "\n",
        "z2 = tf.add(tf.matmul(X, Weight_NN[\"W2\"]), Bias_NN[\"B2\"])\n",
        "z2_out = tf.nn.sigmoid(z2)\n",
        "\n",
        "z3 = tf.add(tf.matmul(X, Weight_NN[\"W3\"]), Bias_NN[\"B3\"])\n",
        "z3_out = tf.nn.sigmoid(z3)\n",
        "\n",
        "z4 = tf.add(tf.matmul(X, Weight_NN[\"W4\"]), Bias_NN[\"B4\"])\n",
        "z4_out = tf.nn.sigmoid(z4)\n",
        "\n",
        "z5 = tf.add(tf.matmul(X, Weight_NN[\"W5\"]), Bias_NN[\"B5\"])\n",
        "NN_out = tf.nn.sigmoid(z5)\n",
        "\n",
        "#implementeng nn using tesorflow api\n",
        "z1 = tf.layers.dense(X,n_hidden_1, activation = tf.nn.sigmoid)\n",
        "\n",
        "z2 = tf.layers.dense(z1,n_hidden_2, activation = tf.nn.sigmoid)\n",
        "z3 = tf.layers.dense(z2,n_hidden_1, activation = tf.nn.sigmoid)\n",
        "z4 = tf.layers.dense(z3,n_hidden_1, activation = tf.nn.sigmoid)\n",
        "\n",
        "NN_out = tf.layers.dense(z4, output_layer)\n",
        "\n",
        "#data preperation\n",
        "\n",
        "#shuffling data set to avoid any bias\n",
        "\n",
        "np.random.shuffle(img_database)\n",
        "X_train = img_database\n",
        "\n",
        "#normalize the data se\n",
        "\n",
        "X_train = X_train\n",
        "\n",
        "#create a noisy data set\n",
        "X_train_noisy = X_train + 10*np.random.normal(0,1, size = X_train.shape)\n",
        "\n",
        "#original image\n",
        "plt.imshow(X_train[0].reshape(28,28), cmap = 'gray')\n",
        "plt.show()\n",
        "\n",
        "#noisy image\n",
        "#\n",
        "plt.imshow(X_train_noisy[0].reshape(28,28), cmap = 'gray')\n",
        "plt.show()\n",
        "\n",
        "#define the metrics\n",
        "\n",
        "#loss function\n",
        "computed_loss = tf.reduce_mean(tf.square(NN_out - Y))\n",
        "\n",
        "#defining optimizer\n",
        "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(computed_loss)\n",
        "\n",
        "#Initialise the variables\n",
        "init = tf.global_variables_initializer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}